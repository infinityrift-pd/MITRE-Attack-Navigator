{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0NoTaPbRYDwO1dMr7WC53"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MITRE ATT&CK Layer Generator\n",
        "This script processes Jira and Splunk data to generate MITRE ATT&CK Navigator layers for security technique visualization. It combines security incidents from different data sources and maps them to MITRE ATT&CK techniques."
      ],
      "metadata": {
        "id": "swcWC7HQ9GY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osASH-sT8ovZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Alter distinct values of Technique Status to Continuous. Apply gradient thresholds\n",
        "# TODO: Create UI as Splunk dashboard - upload files, select platform names, input layer name, select MITRE version\n",
        "# TODO: Create mapping of WOW domain to platform list. Choosing WOW Domain prefilles selected platform lists\n",
        "# TODO: Upload generated MITRE .json file to Git repo rather than creating \"local\" copy (File saved to /content dir)\n",
        "##############################\n",
        "# DONE: Account for Splunk rule_name values with missing Issue key fields (eg 1087?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrRb_o0PQzKX"
      },
      "outputs": [],
      "source": [
        "# CELL 1: Imports and Function Definitions\n",
        "\n",
        "# Install required packages\n",
        "!pip install numpy pandas mitreattack-python\n",
        "\n",
        "from google.colab import drive, files\n",
        "import pandas as pd\n",
        "import pprint\n",
        "from mitreattack.navlayers import Layer\n",
        "from IPython.display import clear_output\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "clear_output()\n",
        "\n",
        "\n",
        "def process_jira_data(filename):\n",
        "  \"\"\"Processes Jira export data to extract relevant labels and combine them.\n",
        "\n",
        "  Args:\n",
        "    filename: The path to the Jira export CSV file.\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame with 'Issue key', 'Status', and 'Combined Labels' columns.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename)\n",
        "\n",
        "  # Nested function to check if a label starts with 'T' followed by a number\n",
        "  # Used to extract Mitre Techniques from Labels fields in csv\n",
        "  def starts_with_T_number(label):\n",
        "    try:\n",
        "      return label.startswith('T') and label[1].isdigit()\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  # Select all columns that start with 'Labels'\n",
        "  # Jira Export have high number of Labels fields\n",
        "  labels_df = df.filter(like='Labels')\n",
        "\n",
        "  # Apply the function to each value in labels_df, keeping only True values\n",
        "  for col in labels_df.columns:\n",
        "    labels_df.loc[:, col] = labels_df[col].apply(lambda x: x if starts_with_T_number(x) else None)\n",
        "\n",
        "  # Drop columns with all NaN values\n",
        "  labels_df = labels_df.dropna(axis=1, how='all')\n",
        "\n",
        "  # Join Issue key and Status to df\n",
        "  df_selected = df[['Issue key', 'Status']].join(labels_df)\n",
        "\n",
        "  # Drop columns with all NaN values from the final DataFrame\n",
        "  # Dropping again as faced issues with empty values\n",
        "  df_selected = df_selected.dropna(axis=1, how='all')\n",
        "\n",
        "  # Drop rows with all NaN values in the label columns from the final DataFrame\n",
        "  df_selected = df_selected.dropna(subset=df_selected.columns[2:], how='all')\n",
        "\n",
        "  # Merge all Labels values into one column \"Combined Labels\", separated by commas\n",
        "  df_selected['Combined Labels'] = df_selected.filter(like='Labels').apply(\n",
        "      lambda row: ', '.join([str(label) for label in row if pd.notna(label)]), axis=1\n",
        "  )\n",
        "\n",
        "  # Drop the original 'Labels' columns\n",
        "  df_selected = df_selected.drop(columns=[col for col in df_selected.filter(like='Labels').columns if col != 'Combined Labels'])\n",
        "\n",
        "  return df_selected\n",
        "\n",
        "def process_splunk_data(filename):\n",
        "  \"\"\"Processes Splunk export data to extract and filter relevant information.\n",
        "\n",
        "  Args:\n",
        "    filename: The path to the Splunk export CSV file.\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame with 'Issue key', 'Status', and 'Combined Labels' columns.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename)\n",
        "\n",
        "  # Select desired columns and rename them\n",
        "  df_selected = df[['jira_story', 'notable_status', 'mitre_tech', \"rule_name\"]]\n",
        "  df_selected = df_selected.rename(columns={\n",
        "      'jira_story': 'Issue key',\n",
        "      'notable_status': 'Status',\n",
        "      'mitre_tech': 'Labels'\n",
        "  })\n",
        "\n",
        "  # Identify rule names with empty 'Issue key'\n",
        "  empty_issue_mask = df_selected['Issue key'].isnull()\n",
        "\n",
        "  # Create dummy Issue keys for rows with empty 'Issue key'\n",
        "  if empty_issue_mask.any():\n",
        "    # Get rule names for empty Issue keys\n",
        "    rule_names_no_issue_key = df_selected[empty_issue_mask]['rule_name'].tolist()\n",
        "    print(f'\\nRule names with empty Issue keys: ({len(rule_names_no_issue_key)})')\n",
        "    print(rule_names_no_issue_key)\n",
        "\n",
        "\n",
        "    # Generate unique dummy Issue keys\n",
        "    for idx, (index, row) in enumerate(df_selected[empty_issue_mask].iterrows(), 1):\n",
        "      dummy_key = f\"Splunk{idx+1:04d} - {row['rule_name']}\"\n",
        "      df_selected.at[index, 'Issue key'] = dummy_key\n",
        "\n",
        "  # Function to check if a string is a valid MITRE technique ID\n",
        "  def is_valid_technique(label):\n",
        "    try:\n",
        "      # Check if the label starts with 'T'\n",
        "      if not label.startswith('T'):\n",
        "        return False\n",
        "\n",
        "      # Check if the rest of the string contains only digits\n",
        "      technique_number = label[1:]\n",
        "      return technique_number.isdigit()\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  # Function to process and clean technique strings\n",
        "  def clean_techniques(tech_string):\n",
        "    if pd.isna(tech_string):\n",
        "      return None\n",
        "\n",
        "    # Split the string by commas and clean each technique\n",
        "    techniques = [t.strip() for t in str(tech_string).split(',')]\n",
        "    # Filter only valid techniques\n",
        "    valid_techniques = [t for t in techniques if is_valid_technique(t)]\n",
        "\n",
        "    # Return None if no valid techniques remain\n",
        "    return ','.join(valid_techniques) if valid_techniques else None\n",
        "\n",
        "  # Apply the function to the 'Labels' column, keeping only valid labels\n",
        "  df_selected['Labels'] = df_selected['Labels'].apply(clean_techniques)\n",
        "\n",
        "  # Drop rows with all NaN values in the label columns\n",
        "  df_selected = df_selected.dropna(subset=['Labels'], how='all')\n",
        "\n",
        "  # Identify rule names with empty 'Issue key'\n",
        "  rule_names_no_issue_key = df_selected[df_selected['Issue key'].isnull()]['rule_name'].tolist()\n",
        "  print(f'\\nRule names with empty Issue keys: ({len(rule_names_no_issue_key)})')\n",
        "  print(rule_names_no_issue_key)\n",
        "\n",
        "  # Remove rows with empty 'Issue key' from df_selected\n",
        "  df_selected = df_selected.dropna(subset=['Issue key'])\n",
        "  df_selected.drop(columns='rule_name', inplace=True)\n",
        "  df_selected = df_selected.rename(columns={'Labels': 'Combined Labels'})\n",
        "\n",
        "  return df_selected\n",
        "\n",
        "def merge_dataframes(jira_df, splunk_df):\n",
        "  \"\"\"Merges Jira and Splunk dataframes and combines duplicate entries.\n",
        "\n",
        "  Args:\n",
        "    jira_df: The pandas DataFrame containing Jira data.\n",
        "    splunk_df: The pandas DataFrame containing Splunk data.\n",
        "\n",
        "  Returns:\n",
        "    A merged pandas DataFrame with combined data.\n",
        "  \"\"\"\n",
        "  # Merge dataframes\n",
        "  merged = pd.concat([jira_df, splunk_df]).groupby('Issue key').agg({\n",
        "      'Status': lambda x: ', '.join(x.dropna().unique()),\n",
        "      'Combined Labels': lambda x: ', '.join(x.dropna().unique())\n",
        "  }).reset_index()\n",
        "\n",
        "  return merged.sort_values('Issue key')\n",
        "\n",
        "def generate_technique_list(final_df):\n",
        "  \"\"\"Generates a list of techniques with their scores and associated issue keys.\n",
        "\n",
        "  Args:\n",
        "    final_df: The merged pandas DataFrame.\n",
        "\n",
        "  Returns:\n",
        "    A list of dictionaries, where each dictionary represents a technique.\n",
        "  \"\"\"\n",
        "  # Status-to-score mapping\n",
        "  status_score_map = {\n",
        "      # Jira Status'\n",
        "      'Backlog': 1,\n",
        "      'To Do': 1,\n",
        "      'In Progress': 1,\n",
        "      'Pending': 1,\n",
        "      'Blocked': 1,\n",
        "      'Done': 2,\n",
        "\n",
        "      # Splunk Status'\n",
        "      'senttophantom': 1,\n",
        "      'new': 2,\n",
        "      'development': 2,\n",
        "      'senttophantom': 1,\n",
        "      'soar_to_triage': 1,\n",
        "      'customer_action': 1,\n",
        "      'closed': 2\n",
        "  }\n",
        "\n",
        "  # Function to get the maximum score based on status\n",
        "  def get_max_score(status_str):\n",
        "    scores = [status_score_map.get(status.strip(), 0) for status in status_str.split(',')]\n",
        "    return max(scores)\n",
        "\n",
        "    # Function to extract rule name from Splunk issue key\n",
        "  def get_rule_name(issue_key):\n",
        "    if isinstance(issue_key, str) and issue_key.startswith('Splunk'):\n",
        "      # Extract everything after \"SplunkXXXX- \"\n",
        "      return issue_key[issue_key.find('-')+2:]\n",
        "    return issue_key\n",
        "\n",
        "  # Extract unique techniques and their associated issue keys\n",
        "  tech_issue_map = {}\n",
        "\n",
        "  for index, row in final_df.iterrows():\n",
        "    issue_key = row['Issue key']\n",
        "    # Split and strip techniques, then convert to set to remove duplicates\n",
        "    techniques = set(t.strip() for t in row['Combined Labels'].split(','))\n",
        "\n",
        "    for technique in techniques:\n",
        "      if technique not in tech_issue_map:\n",
        "        tech_issue_map[technique] = set()\n",
        "      tech_issue_map[technique].add(issue_key)\n",
        "\n",
        "  # Create the final list of dictionaries with aggregated issue keys\n",
        "  tech_list_to_dic = []\n",
        "  for technique, issue_keys in tech_issue_map.items():\n",
        "    # Convert issue_keys to list and sort\n",
        "    issue_keys_list = sorted(issue_keys)\n",
        "\n",
        "    # Handle duplicate Splunk rule names\n",
        "    seen_rules = set()\n",
        "    deduplicated_keys = []\n",
        "\n",
        "    for key in issue_keys_list:\n",
        "      if key.startswith('Splunk'):\n",
        "        rule_name = get_rule_name(key)\n",
        "        if rule_name not in seen_rules:\n",
        "          seen_rules.add(rule_name)\n",
        "          deduplicated_keys.append(key)\n",
        "      else:\n",
        "        # Keep non-Splunk keys as they are\n",
        "        deduplicated_keys.append(key)\n",
        "\n",
        "    max_score = max([get_max_score(final_df.loc[final_df['Issue key'] == key, 'Status'].values[0]) for key in deduplicated_keys])\n",
        "\n",
        "    tech_list_to_dic.append({\n",
        "        'techniqueID': technique,\n",
        "        'score': max_score,\n",
        "        'enabled': True,\n",
        "        'comment': ', '.join(sorted(deduplicated_keys))\n",
        "    })\n",
        "\n",
        "  return tech_list_to_dic\n",
        "\n",
        "def create_mitre_layer(tech_list_to_dic, layer_name, platform_list):\n",
        "  \"\"\"Creates a MITRE ATT&CK Navigator layer with the given techniques and platforms.\n",
        "\n",
        "  Args:\n",
        "    tech_list_to_dic: The list of technique dictionaries.\n",
        "    layer_name: The desired name for the layer.\n",
        "    platform_list: A list of platforms to include in the layer.\n",
        "  \"\"\"\n",
        "\n",
        "  # New layer configuration\n",
        "  description = \"\"\n",
        "\n",
        "  # Create a new layer for the MITRE ATT&CK Navigator\n",
        "  new_layer = Layer()\n",
        "  new_layer.from_dict(dict(name=layer_name, domain=\"enterprise-attack\"))\n",
        "\n",
        "  # Configure the versions object\n",
        "  new_layer.layer.versions = dict(layer=\"4.5\", attack=\"16\", navigator=\"5.0.1\")\n",
        "\n",
        "  # Configure the filters object\n",
        "  new_layer.layer.filters = dict(platforms=platform_list)\n",
        "\n",
        "  # Configure the layout object\n",
        "  new_layer.layer.layout = dict(\n",
        "      layout=\"side\",\n",
        "      showID=False,\n",
        "      showName=True,\n",
        "      showAggregateScores=False,\n",
        "      countUnscored=False,\n",
        "      aggregateFunction=\"average\"\n",
        "  )\n",
        "\n",
        "  # Configure whether or not to hide disabled techniques\n",
        "  new_layer.layer.hideDisabled = False\n",
        "\n",
        "  # Configure the gradient object\n",
        "  new_layer.layer.gradient = dict(\n",
        "      minValue=0,\n",
        "      maxValue=2,\n",
        "      colors=[\"#ff6666ff\", \"#ffe766ff\", \"#8ec843ff\"]\n",
        "  )\n",
        "\n",
        "  # Configure collection layer settings\n",
        "  new_layer.layer.description = description\n",
        "  new_layer.layer.selectTechniquesAcrossTactics = True\n",
        "  new_layer.layer.selectSubtechniquesWithParent = False\n",
        "  new_layer.layer.tacticRowBackground = \"#dddddd\"\n",
        "\n",
        "  # Create listing of techniques in this layer\n",
        "  new_layer.layer.techniques = tech_list_to_dic\n",
        "\n",
        "  # Output file is `layer_name`, with .json extension\n",
        "  new_layer.to_file(layer_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: User Input and Data Processing\n",
        "\n",
        "# Get user input for file types\n",
        "file_types = input(\"Which files do you want to upload? (splunk, jira, both): \").lower()\n",
        "\n",
        "# Define the platform list\n",
        "all_platforms = [\"Windows\", \"Linux\", \"macOS\", \"Network\", \"PRE\", \"Containers\", \"IaaS\", \"SaaS\", \"Office Suite\", \"Identity Provider\"]\n",
        "\n",
        "# Get user input for platforms\n",
        "print(\"Available platforms:\")\n",
        "# Display platforms in a numbered table for user's reference\n",
        "platform_data = [[i+1, platform] for i, platform in enumerate(all_platforms)]\n",
        "print(pd.DataFrame(platform_data, columns=[\"Number\", \"Platform\"]).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# User to enter in corresponding number for selected platforms\n",
        "platform_input = input(\"Enter the numbers of the platforms to include (comma-separated, or leave blank for all): \")\n",
        "\n",
        "if platform_input:\n",
        "  try:\n",
        "    platform_indices = [int(x.strip()) - 1 for x in platform_input.split(\",\")]\n",
        "    platform_list = [all_platforms[i] for i in platform_indices]\n",
        "  except:\n",
        "    print(\"Invalid platform input. Using all platforms.\")\n",
        "    platform_list = all_platforms\n",
        "else:\n",
        "  platform_list = all_platforms\n",
        "\n",
        "# Get layer name from user\n",
        "layer_name = input(\"Enter the desired layer name: \")\n",
        "# Add date to layer name\n",
        "today = datetime.date.today()\n",
        "layer_name = f\"{today.strftime('%y-%m-%d')}_{layer_name}.json\"\n",
        "\n",
        "# Process data based on user input\n",
        "if file_types == \"splunk\":\n",
        "  splunk_filename = '/content/splunk_export_csv.csv'  # TEST FILE USED. CHANGE TO DESIRED FILENAME SCHEME\n",
        "  splunk_df = process_splunk_data(splunk_filename)\n",
        "  final_df = splunk_df  # No merging needed\n",
        "elif file_types == \"jira\":\n",
        "  jira_filename = '/content/jira_mitre_csv.csv'  # TEST FILE USED. CHANGE TO DESIRED FILENAME SCHEME\n",
        "  jira_df = process_jira_data(jira_filename)\n",
        "  final_df = jira_df  # No merging needed\n",
        "elif file_types == \"both\":\n",
        "  jira_filename = '/content/jira_mitre_csv.csv'  # TEST FILE USED. CHANGE TO DESIRED FILENAME SCHEME\n",
        "  splunk_filename = '/content/splunk_export_csv.csv'  # TEST FILE USED. CHANGE TO DESIRED FILENAME SCHEME\n",
        "  jira_df = process_jira_data(jira_filename)\n",
        "  splunk_df = process_splunk_data(splunk_filename)\n",
        "  final_df = merge_dataframes(jira_df, splunk_df)\n",
        "else:\n",
        "  print(\"Invalid file type input.\")\n",
        "  exit()\n"
      ],
      "metadata": {
        "id": "bQBOvf6qWdQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c07fdff-e510-4bbc-bf3f-9778c978c465",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Which files do you want to upload? (splunk, jira, both): both\n",
            "Available platforms:\n",
            "| Number   | Platform          |\n",
            "|:---------|:------------------|\n",
            "| 1        | Windows           |\n",
            "| 2        | Linux             |\n",
            "| 3        | macOS             |\n",
            "| 4        | Network           |\n",
            "| 5        | PRE               |\n",
            "| 6        | Containers        |\n",
            "| 7        | IaaS              |\n",
            "| 8        | SaaS              |\n",
            "| 9        | Office Suite      |\n",
            "| 10       | Identity Provider |\n",
            "Enter the numbers of the platforms to include (comma-separated, or leave blank for all): \n",
            "Enter the desired layer name: test_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-df71c58fc876>:26: DtypeWarning: Columns (22,24,27,44,45,46,47,48,57,64,66,70,77,78,79,80,84,85,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,114,116,117,118,125,126,132,133,149,150,151,152,1071,1301,1304,1305,1308,1823,1925,2074,2077,2184,2301,2800,2850) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(filename)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rule names with empty Issue keys: (249)\n",
            "['ana test correlation search', 'ana test correlation search', 'rwds - auth0 - bfc - impossible travel', 'rwds - auth0 - bfc - known malicious ip', 'rwds - auth0 - bfc - successful login from device accessing multiple accounts', 'rwds - auth0 - bfc - successful login from device with multiple failed logins', 'wow - auth0 - tactical inc13177313', 'wowga - ehop - extrahop unified detections', 'aws rds lcdm database failed authentication attempts', 'wowga - aws - detected creation/update of an inline policy for a user/group', 'aws rds lcdm database user account is created/modified/deleted', 'attempt to bypass conditional access rule in azure ad', 'azure ad added to privileged group', 'canary disconnected', 'crowdstrike detection', 'crowdstrike incident', 'crowdstrike t0 violation', 'crowdstrike detection of utilman abuse', 'detect multiple suspicious command-line arguments', 'gws - detect for the collection of users emails', 'gws - detect for the retrieval of accounts, groups users and applications', 'gws - email forwarding to external domains', 'gws - email forwarding to external domains', 'excessive failed logins on crowdstrike', 'excessive failed logins on crowdstrike', 'gws - detection for the removal/change of mfa', 'gws - detection for the removal/change of mfa', 'gws - potential user account takeover', 'google workspace added to privileged group', 'google workspace admin changes', 'network - potential vulnerability scanner was detected (by event)', 'network - potential vulnerability scanner was detected (by targets)', 'powershell used for credential dumping via lsass.exe', 'sec_cs_endpoint_detect wmi event subscription persistence', 'sec_cs_endpoint_unexpected parent process spawned svchost.exe', 'sec_cs_endpoint_unexpected parent process spawned svchost.exe', 'sec_cs_endpoint_unexpected parent process spawned svchost.exe', 'sec_cs_endpoint_unexpected parent process spawned svchost.exe', 'sec_cs_endpoint_unexpected process spawned svchost.exe', 'sec_cs_endpoint_wmi permanent event subscription - sysmon', 'sec_cs_threat_log4j ioc activity allowed activity detected', 'smbv1 abuse detection', 'soc - detect malicious persistence - time providers', 'soc - detect malicious persistence - time providers', 'soc - detect malicious persistence - time providers', 'soc - detect malicious persistence - time providers', 'soc - threat activity detected', 'source data matching more than one threat intel feed', 'splunk privileged role change', 'network - telstra cleaner pipes - alert', 'wmi event subscription persistence - wmieventfilter', 'wow - aad - detect for bulk creation/joining of devices to azure ad', 'wow - aad - detect for guest accounts added to privileged roles', 'wow - aad - detect for potentially malicious app consent', 'wow - aad - detect for short-lived accounts', 'wow - aad - detect for successful logins using legacy authentication', 'wow - aad - explicit mfa deny', 'wow - azure - internal password spray attempt', 'wow - azure - new azure image created from external source - (sap/corp)prod', 'wow - azure - new azure image created from external source - (sap/corp)prod', 'wow - azure - new azure image created from external subscription/storage', 'wow - azure - azure defender for cloud alert events (corpprod)', 'wow - azure - azure defender for cloud alert events (corpprod)', 'wow - azure - defender alert for malware in storage blobs', 'wow - azure - defender alert for malware in storage blobs', 'wow - azure - detect local admin account manipulation', 'wow - azure - detect local admin account manipulation - (sap/corp)prod', 'wow - azure - detect local admin account manipulation - (sap/corp)prod', 'wow - azure - detect for serverless execution or event triggers', 'wow - azure - detect for serverless execution or event triggers-(sap/corp)prod', 'wow - azure - detect for serverless execution or event triggers-(sap/corp)prod', 'wow - azure - detect for change of network security rules', 'wow - azure - detect for change of network security rules (sapprod)', 'wow - azure - detect for change of network security rules (sapprod)', 'wow - azure - detect for the deletion of keyvaults', 'wow - azure - detect for the deletion of diagnostic logs', 'wow - azure - detect for the deletion of diagnostic logs - (sap/corp)prod', 'wow - azure - detect for the deletion of diagnostic logs - (sap/corp)prod', 'wow - azure - detection for creation and deletion of cloud instances', 'wow - azure - detection for creation and deletion of cloud instances-(sap/corp)prod', 'wow - azure - detection for creation and deletion of cloud instances-(sap/corp)prod', 'wow - azure - detection of unmanaged scripting', 'wow - azure - detection of unmanaged scripting - (sap/corp)prod', 'wow - azure - detection of unmanaged scripting - (sap/corp)prod', 'wow - azure - detection of user execution via vm scripting- (sap/corp)prod', 'wow - azure - detection of user execution via vm scripting- (sap/corp)prod', 'wow - azure - detection of user execution via virtual machine scripting', 'wow - canry - detection for canary honeypot alert', 'wow - canry - detection for canary token alert', 'wow - cstrk - detect unauthorised remote access tool use', 'wow - cstrk - generate notable from crowdstrike scheduled search', 'wow - gws - account warning login events', 'wow - gws - detect for documents in google drive to be made external/public', 'wow - gws - detect for documents in google drive to be made public', 'wow - gws - detection for suspicious saml or token authentication events', 'wow - sfdc - report export activity alert', 'wow - zia - sectmr:4325', 'wow - zpa - malicious internet traffic', 'wow-aad-detect for consecutive account enablement and password reset events', 'wow-aad-detect for domain policy modifications in azure ad', 'wow-aad-successful authentication on azuread from user with unusual location', 'wow-auth0-successful impossible travel for auth0', 'wow-azure-detect for the backup of a instance', 'wow-gws-detect gmail accounts with leaked password', 'wow-gws-suspicious login events', 'wow-orca- capture data risk orca alerts as notable events', 'wow-win-ad member removed from group', 'wow-win-ad member removed from group', 'wow-win-ad member removed from group', 'wowga - cstrk - crowdstrike rtr usage', 'wow-azure-detect for the creation of resources outside of australia', 'zscaler casb salesforce detection', 'allowed log4j ioc at akamai', 'detect user added to local group', 'detect user or spn added to azure privileged roles', 'from escu 3.35.0: detect spike in s3 bucket deletion', 'new identity added to azure global administrator role', 'zscaler advanced threat super category allow', 'critical and high priority alerts from alerts datamodel', 'wowga - indicator - detected change to saved search', 'alert: detect for when a local account is enabled - stores', 'alert: detect for when a local account is enabled - stores', 'alert: detect for when a new local account is created - stores', 'alert: detect for when a new local account is created - stores', 'remote wmi command attempt - stores', 'sec_cs_endpoint_unexpected parent process spawned svchost.exe', 'soc - detect malicious persistence - time providers', 'soc - detect malicious persistence - time providers', 'stores - aad - azure powershell usage from store user', 'wow-aad-detect stores shared accounts accessed from nonstore locations', 'wow-win-ad member removed from group', 'wow-win-ad member removed from group', 'xsl script processing detected in stores', 'xsl script processing detected in stores', 'aws rds lcdm database failed authentication attempts', 'aws rds lcdm database user account is created/modified/deleted', 'administrator type account is successfully used on the rewards aws ec2 bastion host', 'detect when a rewards aws consolelogin occurs without mfa', 'detect when a rewards aws consolelogin occurs without mfa', 'rwds - api - anomalous increase in preference changes to automatic', 'rwds - api - detect for sudden increase in otp generation/login attempts', 'rwds-api-automatic preference changes by single ip on multiple accounts', 'rwds-api-detect excessive failed logon attempts into a rewards member account', 'rwds-api-detect requests using addname parameter to display card number', 'rwds-api-detect sudden increase in rewards otp verification failures', \"wowga - aws - access control list ('acl') is created with all open ports\", 'wowga - aws - detect access control list deleted', 'wowga - aws - detect createdevendpoint and getdevendpoint commands are called', 'wowga - aws - detect database cloned', 'wowga - aws - detect ec2 instances stopped or terminated successfully', 'wowga - aws - detect encryption key disabled or scheduled for deletion', 'wowga - aws - detect sharedsnapshotvolume created and run on ec2 instance', 'wowga - aws - detect snapshot is created or attached to an ec2 instance', 'wowga - aws - detected creation/update of an inline policy for a user/group', 'wowga - aws - detected creation/update of an inline policy for a user/group', 'wowga - aws - passrole, createfunction, invokefunction used to escalate privileges', 'wowga - aws - user, login profile, policy, or group deleted or removed', 'from escu 3.35.0: detect spike in s3 bucket deletion', 'mdeal - cstrk - detect crowdstrike incidents', 'mdeal - jc - detect rogue devices added to mydeal jc', \"wowga - aws - access control list ('acl') is created with all open ports\", \"wowga - aws - access control list ('acl') is created with all open ports\", 'wowga - aws - detect access control list deleted', 'wowga - aws - detect access control list deleted', 'wowga - aws - detect createdevendpoint and getdevendpoint commands are called', 'wowga - aws - detect createdevendpoint and getdevendpoint commands are called', 'wowga - aws - detect database cloned', 'wowga - aws - detect database cloned', 'wowga - aws - detect ec2 instances stopped or terminated successfully', 'wowga - aws - detect ec2 instances stopped or terminated successfully', 'wowga - aws - detect encryption key disabled or scheduled for deletion', 'wowga - aws - detect encryption key disabled or scheduled for deletion', 'wowga - aws - detect sharedsnapshotvolume created and run on ec2 instance', 'wowga - aws - detect sharedsnapshotvolume created and run on ec2 instance', 'wowga - aws - detect snapshot is created or attached to an ec2 instance', 'wowga - aws - detect snapshot is created or attached to an ec2 instance', 'wowga - aws - detected creation/update of an inline policy for a user/group', 'wowga - aws - passrole, createfunction, invokefunction used to escalate privileges', 'wowga - aws - passrole, createfunction, invokefunction used to escalate privileges', 'wowga - aws - user, login profile, policy, or group deleted or removed', 'wowga - aws - user, login profile, policy, or group deleted or removed', 'wowga - cstrk - crowdstrike rtr usage', 'distb - ehop - extrahop detections for dc assets', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect code signing policy modification on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect for netntlm downgrade attack on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - detect install root certificate on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - win - domain trust discovery on windows hosts in dcs', 'distb - zia - malicious internet traffic', 'administrator type accounts successfully used on the bigw aws ec2 instances', 'bigw - aem - detect for account impersonation', 'bigw - aem - detect for user group modification adobe aem', 'wow - azure - new azure image created from external subscription/storage', 'wow - azure - detect local admin account manipulation', 'wow - azure - detect for serverless execution or event triggers', 'wow - azure - detect for change of network security rules', 'wow - azure - detect for the deletion of keyvaults', 'wow - azure - detect for the deletion of diagnostic logs', 'wow - azure - detection for creation and deletion of cloud instances', 'wow - azure - detection of unmanaged scripting', 'wow - azure - detection of user execution via virtual machine scripting', 'wow-azure-detect for the backup of a instance', \"wowga - aws - access control list ('acl') is created with all open ports\", 'wowga - aws - detect access control list deleted', 'wowga - aws - detect createdevendpoint and getdevendpoint commands are called', 'wowga - aws - detect database cloned', 'wowga - aws - detect ec2 instances stopped or terminated successfully', 'wowga - aws - detect encryption key disabled or scheduled for deletion', 'wowga - aws - detect sharedsnapshotvolume created and run on ec2 instance', 'wowga - aws - detect snapshot is created or attached to an ec2 instance', 'wowga - aws - detect for new user without enforcemfa group', 'wowga - aws - detected creation/update of an inline policy for a user/group', 'wowga - aws - passrole, createfunction, invokefunction used to escalate privileges', 'wowga - aws - user, login profile, policy, or group deleted or removed', 'wow-azure-detect for the creation of resources outside of australia']\n",
            "\n",
            "Rule names with empty Issue keys: (0)\n",
            "[]\n"
          ]
        }
      ]
    }
  ]
}